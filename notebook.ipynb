{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis notebook demonstrate a machine learning model project prototyping from [Grab AI Challengle in Traffic Management](https://www.aiforsea.com/traffic-management) as a starting kit.\n\n## Objective\n\n> Economies in Southeast Asia are turning to AI to solve traffic congestion, which hinders mobility and economic growth. The first step in the push towards alleviating traffic congestion is to understand travel demand and travel patterns within the city.\n> \n> Can we accurately forecast travel demand based on historical Grab bookings to predict areas and times with high travel demand?"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install pendulum","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install fbprophet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport Geohash\nfrom matplotlib import pyplot as plt\nfrom math import sin, cos, sqrt, atan2, radians\nimport pendulum\nfrom scipy.interpolate import interpn\nfrom sklearn.preprocessing import LabelBinarizer , LabelEncoder, OneHotEncoder, MinMaxScaler\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom numpy import split\nfrom numpy import array\nfrom sklearn.metrics import mean_squared_error\n\nimport utils\nimport gc # garbage collector\nimport seaborn as sns\n% matplotlib inline\nplt.style.use('seaborn-whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import fbprophet\nfbprophet.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly\nplotly.__version__\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploratory and wrangling\nFirst step in machine learning is always about understanding the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/training.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for null values\ndf[df.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decoding geohash6\nWe will do a decoding into latitude and longitude back this encoding to find out more information about the the data location.\n> Geohash is a public domain geocoding system invented by Gustavo Niemeyer[1], which encodes a geographic location into a short string of letters and digits. It is a hierarchical spatial data structure which subdivides space into buckets of grid shape, which is one of the many applications of what is known as a Z-order curve, and generally space-filling curves (source: wikipedia).\n\nMore information about the geohash can be found in wikipedia: [wikipedia](https://en.wikipedia.org/wiki/Geohash): \n\n> So geohash6 means that it has 6 characters with the precision of ±0.61km (610m). I suspect landmark or point of interest is already compressed into this single encoding value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the unique hashes\ngeohashes_df = df.groupby('geohash6', as_index=False)\\\n.agg({'day':'count'})\\\n.rename(columns={'day':'count'})\\\n.sort_values(by='count', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total encoded hash\",len(geohashes_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"geohashes_df.sort_values(by='count', ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find out the distribution of the hash\nax = sns.distplot(geohashes_df['count'], rug=True, hist=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# decode geohash into lat and long\ngeohashes_df['lat'] = None\ngeohashes_df['lat_err'] = None\ngeohashes_df['long'] = None\ngeohashes_df['long_err'] = None\nfor i in range(len(geohashes_df)):\n    geo_decoded = Geohash.decode_exactly(geohashes_df.loc[i,'geohash6'])\n    geohashes_df.loc[i,'lat'] = geo_decoded[0]\n    geohashes_df.loc[i,'long'] = geo_decoded[1]\n    geohashes_df.loc[i,'lat_err'] = geo_decoded[2]\n    geohashes_df.loc[i,'long_err'] = geo_decoded[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 10 geo\ngeohashes_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge into original df\ndf = df.merge(geohashes_df.drop(columns=['count']), on='geohash6', how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert lat and long into float type\ndf['lat'] = df['lat'].astype('float64')\ndf['long'] = df['long'].astype('float64')\ndf['lat_err'] = df['lat_err'].astype('float64')\ndf['long_err'] = df['long_err'].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Timestamp and day\nThe timestamp is not a linux timestamp but rather a `hour:minute` form. Another thing we don't have is the context of  month and year. But we'll make an attempt to deduce. Knowing which month and year could help us in forming seasonality or holiday that potentially affecting the prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract hour and minute from timestamp column\ndf[['h','m']] = df['timestamp'].str.split(':',expand=True)\ndf['h'] = df['h'].astype('int64')\ndf['m'] = df['m'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract day of week (DoW) from day\n# since we have no idea about which month is this data were taken so can't\n# be sure the DoW starts on which day\n# but it's good enought we dont need to dig deeper I supposed\n# but in test set clearly this thing needs to be mapped out correcly\ndf['dow'] = df['day'] % 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# outlier from day\nzscore = lambda x: (x - x.mean()) / x.std()\ndf['zscore_day'] = np.abs(df.groupby('day')['demand'].transform(zscore))\nprint(\"number of suspected outliers from day\", len(df[df['zscore_day'] > 3]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# outlier from timestamp\ndf['zscore_timestamp'] = np.abs(df.groupby('timestamp')['demand'].transform(zscore))\nprint(\"number of suspected outliers from timestamp\", len(df[df['zscore_timestamp'] > 3]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = df[(df['zscore_day'] <= 3) & (df['zscore_timestamp'] <= 3)].groupby(['dow','h'], as_index=False)\\\n.agg({'demand':'mean'})\\\n.sort_values(by=['dow','h'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [\n    go.Heatmap(\n        z=_['demand'].values.reshape((7,24)),\n        x=[str(i) for i in range(24)],\n        y=[str(i) for i in range(7)],\n        #colorscale='Viridis',\n    )\n]\n\nlayout = go.Layout(\n    title='Average Demand in Day of Week x 24 hours',\n    xaxis=dict(\n        title='Hours',\n    ),\n    yaxis=dict(\n        title='Week of Day',\n    ),\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":161,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n        \n        \n            <div id=\"4b442aa4-37a9-49fa-be86-c9d770f79f38\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n            <script type=\"text/javascript\">\n                require([\"plotly\"], function(Plotly) {\n                    window.PLOTLYENV=window.PLOTLYENV || {};\n                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n                    \n                if (document.getElementById(\"4b442aa4-37a9-49fa-be86-c9d770f79f38\")) {\n                    Plotly.newPlot(\n                        '4b442aa4-37a9-49fa-be86-c9d770f79f38',\n                        [{\"type\": \"heatmap\", \"uid\": \"ff05b55d-0725-4c1c-946d-91c4c037e6f1\", \"x\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\"], \"y\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"], \"z\": [[0.08320858351356024, 0.08705090870018056, 0.08485022722986468, 0.08971301441573322, 0.09532127837056824, 0.09626930533495616, 0.09508747613888348, 0.09350099501512442, 0.09875958583272891, 0.10388736765788414, 0.10482497083032433, 0.09930645291919293, 0.09380071680950451, 0.08704619983133036, 0.07849862762904102, 0.06467643005181889, 0.05444011519861326, 0.04424869014266345, 0.03804492342904057, 0.034011172560117583, 0.0340819733884904, 0.04532378948614668, 0.05390508576832002, 0.0706587423781917], [0.07921177139483315, 0.08430943445817837, 0.08439530940226314, 0.08891103451553897, 0.09542986156743696, 0.09635980017421202, 0.09486517331776977, 0.0931257024752527, 0.09571634109426726, 0.10017731447211953, 0.10173266973025474, 0.09798467333505931, 0.09315774860983983, 0.0867828163890741, 0.07986589063898208, 0.06522301753808803, 0.053950860322068424, 0.04460495540905389, 0.038717064240441704, 0.03500874059621295, 0.03400518150517998, 0.04512619784320614, 0.05399181554535962, 0.07251476933224534], [0.08482296370027546, 0.08887423226012857, 0.08744326845546352, 0.09065662161920288, 0.09706881070352066, 0.10035488662831132, 0.098150010627375, 0.09735357559673802, 0.10066081642399875, 0.10715503538485366, 0.10798449584748135, 0.10332167973333808, 0.09672513127269376, 0.09178125412342843, 0.08454409419124154, 0.0696636031748962, 0.057799482917650725, 0.04663521530227816, 0.040345601614597125, 0.0363558386407176, 0.03613398107882957, 0.0453411854856174, 0.053779873259324845, 0.070643250626801], [0.08284722279720118, 0.08762148993382042, 0.08616767244102241, 0.09031333765097238, 0.09734698390274309, 0.09955556689910047, 0.09835024150482272, 0.09785266057548896, 0.10169249352283839, 0.10945072128701003, 0.11133928898565593, 0.1071113634028237, 0.10102714209470616, 0.09518632881248579, 0.08777975820006002, 0.0713501031533375, 0.05972031326892696, 0.04751104536124798, 0.04169784183508697, 0.038952099249621445, 0.038233888421754514, 0.046901661527263085, 0.053444798600680105, 0.07131487838329763], [0.08113444180239479, 0.08613953489030256, 0.08714706016858918, 0.09241714729331038, 0.10373615233329078, 0.10681719108615587, 0.10613976726231734, 0.10337469291006005, 0.10989537463732653, 0.11648798573415385, 0.11887032445058562, 0.11317419169464504, 0.10642790467803381, 0.09742659162883609, 0.09215641689270625, 0.07780193305650539, 0.06550915594598042, 0.05469524879685136, 0.04711553871812165, 0.0437216328464653, 0.04174257861825671, 0.043215143589779105, 0.04557380167227332, 0.057711101295709735], [0.06371700233971056, 0.07244221469927753, 0.07895378843647803, 0.08614630026206153, 0.09259973036822566, 0.09611093820330603, 0.09535425803372533, 0.09612662346352394, 0.09690160146804797, 0.09656307228882087, 0.09509138715015168, 0.09202749503440308, 0.08867585457141411, 0.08432810320891315, 0.08308459264877498, 0.07307118762361275, 0.0609167214301874, 0.05047945020422038, 0.04425646919222384, 0.040016876917939634, 0.03777919618484406, 0.03716676195852145, 0.037322030410421245, 0.04269997713488785], [0.050965077519664545, 0.061133911828714735, 0.07105818109902798, 0.07566995255095335, 0.07937248142972136, 0.0803432363656493, 0.08109651069494526, 0.07961312205075684, 0.07901968547669658, 0.0812686895424398, 0.0824663781246135, 0.08260877018845518, 0.08100657857597346, 0.07856115453473549, 0.07501132421172856, 0.06286266564089511, 0.052758216186365406, 0.04159168014197128, 0.03463619603629809, 0.030336811974651, 0.031209234537963867, 0.04348399771589804, 0.05303456848198654, 0.06996767149404197]]}],\n                        {\"title\": {\"text\": \"Average Demand in Day of Week x 24 hours\"}, \"xaxis\": {\"title\": {\"text\": \"Hours\"}}, \"yaxis\": {\"title\": {\"text\": \"Week of Day\"}}},\n                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n                    ).then(function(){\n                            \nvar gd = document.getElementById('4b442aa4-37a9-49fa-be86-c9d770f79f38');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })\n                };\n                });\n            </script>\n        </div>","application/vnd.plotly.v1+json":{"config":{"linkText":"Export to plot.ly","plotlyServerURL":"https://plot.ly","responsive":true,"showLink":false},"data":[{"type":"heatmap","uid":"26b17030-b278-48d5-ba8b-27934fd28799","x":["0","1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23"],"y":["0","1","2","3","4","5","6"],"z":[[0.08320858351356024,0.08705090870018056,0.08485022722986468,0.08971301441573322,0.09532127837056824,0.09626930533495616,0.09508747613888348,0.09350099501512442,0.09875958583272891,0.10388736765788414,0.10482497083032433,0.09930645291919293,0.09380071680950451,0.08704619983133036,0.07849862762904102,0.06467643005181889,0.05444011519861326,0.04424869014266345,0.03804492342904057,0.034011172560117583,0.0340819733884904,0.04532378948614668,0.05390508576832002,0.0706587423781917],[0.07921177139483315,0.08430943445817837,0.08439530940226314,0.08891103451553897,0.09542986156743696,0.09635980017421202,0.09486517331776977,0.0931257024752527,0.09571634109426726,0.10017731447211953,0.10173266973025474,0.09798467333505931,0.09315774860983983,0.0867828163890741,0.07986589063898208,0.06522301753808803,0.053950860322068424,0.04460495540905389,0.038717064240441704,0.03500874059621295,0.03400518150517998,0.04512619784320614,0.05399181554535962,0.07251476933224534],[0.08482296370027546,0.08887423226012857,0.08744326845546352,0.09065662161920288,0.09706881070352066,0.10035488662831132,0.098150010627375,0.09735357559673802,0.10066081642399875,0.10715503538485366,0.10798449584748135,0.10332167973333808,0.09672513127269376,0.09178125412342843,0.08454409419124154,0.0696636031748962,0.057799482917650725,0.04663521530227816,0.040345601614597125,0.0363558386407176,0.03613398107882957,0.0453411854856174,0.053779873259324845,0.070643250626801],[0.08284722279720118,0.08762148993382042,0.08616767244102241,0.09031333765097238,0.09734698390274309,0.09955556689910047,0.09835024150482272,0.09785266057548896,0.10169249352283839,0.10945072128701003,0.11133928898565593,0.1071113634028237,0.10102714209470616,0.09518632881248579,0.08777975820006002,0.0713501031533375,0.05972031326892696,0.04751104536124798,0.04169784183508697,0.038952099249621445,0.038233888421754514,0.046901661527263085,0.053444798600680105,0.07131487838329763],[0.08113444180239479,0.08613953489030256,0.08714706016858918,0.09241714729331038,0.10373615233329078,0.10681719108615587,0.10613976726231734,0.10337469291006005,0.10989537463732653,0.11648798573415385,0.11887032445058562,0.11317419169464504,0.10642790467803381,0.09742659162883609,0.09215641689270625,0.07780193305650539,0.06550915594598042,0.05469524879685136,0.04711553871812165,0.0437216328464653,0.04174257861825671,0.043215143589779105,0.04557380167227332,0.057711101295709735],[0.06371700233971056,0.07244221469927753,0.07895378843647803,0.08614630026206153,0.09259973036822566,0.09611093820330603,0.09535425803372533,0.09612662346352394,0.09690160146804797,0.09656307228882087,0.09509138715015168,0.09202749503440308,0.08867585457141411,0.08432810320891315,0.08308459264877498,0.07307118762361275,0.0609167214301874,0.05047945020422038,0.04425646919222384,0.040016876917939634,0.03777919618484406,0.03716676195852145,0.037322030410421245,0.04269997713488785],[0.050965077519664545,0.061133911828714735,0.07105818109902798,0.07566995255095335,0.07937248142972136,0.0803432363656493,0.08109651069494526,0.07961312205075684,0.07901968547669658,0.0812686895424398,0.0824663781246135,0.08260877018845518,0.08100657857597346,0.07856115453473549,0.07501132421172856,0.06286266564089511,0.052758216186365406,0.04159168014197128,0.03463619603629809,0.030336811974651,0.031209234537963867,0.04348399771589804,0.05303456848198654,0.06996767149404197]]}],"layout":{"title":{"text":"Average Demand in Day of Week x 24 hours"},"xaxis":{"title":{"text":"Hours"}},"yaxis":{"title":{"text":"Week of Day"}}}}},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Heatmap analysis\n* We have no context which month the day 0 is\n* But from heatmap I would speculate that the 0 is Monday \n* Since we can observe the stripe on 6th DoW noticably deviating from the rest, this could indicate Sunday\n* Looking at the \"First month\" vs \"second month\" plot, I suspect the dataset were derived from Malaysia and for October and November 2018."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's find out since 2018 which month has monday for their first day\nstart = pendulum.datetime(2018, 1, 1)\nend = pendulum.datetime(2019, 5, 22)\n\nperiod = pendulum.period(start, end)\n\nfor dt in period.range('days'):\n    if dt.day_of_week == pendulum.MONDAY and dt.day == 1:\n        print(dt.to_date_string(), \"number of days:\", dt.end_of('month').day-dt.start_of('month').day+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = df.groupby('day', as_index=False).agg({'demand':'mean'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [i for i in range(31)]\ntrace1 = go.Scatter(\n    x = x,\n    y = _['demand'][0:31],\n    mode = 'lines+markers',\n    name = 'First Month'\n)\ntrace2 = go.Scatter(\n    x = x,\n    y = _['demand'][31:61],\n    mode = 'lines+markers',\n    name = 'Second Month'\n)\n\ndata = [trace1, trace2]\niplot(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demand distribution\ndf['demand'].hist(bins=100, figsize=(14,3))\nplt.xlabel('Demand normalized')\nplt.title('Histogram');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis of the Geo data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_dist(lat1, long1, lat2, long2):\n    # calculate distance between two coordinate using Haversine formula\n    # approximate radius of earth in km\n    R = 6373.0\n    lat1 = radians(lat1)\n    lon1 = radians(long1)\n    lat2 = radians(lat2)\n    lon2 = radians(long2)\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    distance = R * c\n    return distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# minimum and maximum latitude\nlat_min, lat_max = df['lat'].min(), df['lat'].max()\n# minimum and maximum longitude\nlong_min, long_max = df['long'].min(), df['long'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Min coordinate\", (lat_min, long_min))\nprint(\"Max coordinate\", (lat_max, long_max))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lat_A = lat_max\nlong_A = long_min\nlat_B = lat_min\nlong_B = long_max\ndA = calc_dist(lat_min, long_min, lat_A, long_A)\ndB = calc_dist(lat_min, long_min, lat_B, long_B)\ndiameter = calc_dist(lat_min, long_min, lat_max, long_max)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate the bounding area\n* The square area (KM) of the GPS bounding box is $ 1,170 km^2 $ . So I imagine the size close to Penang"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Circle Area\",np.pi * (diameter/2)**2)\nprint(\"Square area\", dA*dB)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot into Map\nHere we want to understand if we can use external data augmentation from the geo location. But turns out the location were masked into ocean. Unless grab is providing on demand hailing to Atlantist, I'm afraid we can't do much use the geo data for augmentation such as landmarks (point of interest)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.openstreetmap.org/#map=4/-6.61/109.16\n# The bounding box in openstreetmap: left, bottom, right, top (min long, min lat, max long, max lat)\nbbox = (89.00, 119.31,-9.15, 10.17) # we'll plot long on x-axis\nsea_map = plt.imread(\"http://madet.my/images/map_sea_trim.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha=0.3\ns=1\nfig, ax = plt.subplots(1, 1, figsize=(16,10))\nax.scatter(df['long'],df['lat'], zorder=1, alpha=alpha, c='r', s=s)\nax.set_xlim((bbox[0], bbox[1]))\nax.set_ylim((bbox[2], bbox[3]))\nax.set_title('SEA Map (partial) with scatter points of demand geolocation')\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\nax.imshow(sea_map, zorder=0, extent=bbox)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nx = df['lat'].values\ny = df['long'].values\nplt.figure(figsize=(14,8))\ndata , x_e, y_e = np.histogram2d( x, y)\nz = interpn( ( 0.5*(x_e[1:] + x_e[:-1]) , 0.5*(y_e[1:]+y_e[:-1]) ) , \n            data , np.vstack([x,y]).T , method = \"splinef2d\", bounds_error = False)\nidx = z.argsort()\nx, y, z = x[idx], y[idx], z[idx]\nplt.scatter(x, y, c=z, cmap='magma')\nplt.colorbar()\nplt.xlabel('Latitude')\nplt.ylabel('Longitude')\nplt.title('Density of demand by location');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Models\n* This is a supervised learning problem where we can throw to the model the features as well as actual values to be learned. \n* Since we have multiple features to be used as forecasting step, it's fall under multivariate forecasting\n* We'll tackle using 2 simple approach here: \n  * Single variate time series forecasting and use parallel training (@todo)\n  * fbprohet multivariate through regressor\n  * Neural network\n  \n  \n* Here what I meant by baseline model is that there is no improvement after first time running the model. In the actual machine learning project one would usually have this similar approach and incrementally fine tune the model to produce better result such as finding best hyperparameter, adding more features, remove outliers, and etc\n* From my experience working with machine learning project major challenge usualy is overfit and imbalance class issues. If we look at the the geohash observation some only got few example so it could potentially overfit and produce far incorrect prediction in production environment. Another thing to observe is outliers, one scenario is sudden spike in certain areas due to a random events. We probably want to exclude this if such spike is not repeatable in seasonality pattern.\n  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# label encode the geohash6\n# though the best way is probably onehot encode, but we'll leave it to next iteration\nlabelencoder = LabelEncoder()\ndf['geo_encoded'] = labelencoder.fit_transform(df['geohash6'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# @TODO encode the geo to onehot or binary\n# lb = LabelBinarizer(sparse_output=False)\n# lb.fit(df['geo_encoded'])\n# we'll transform geohash6 into onehot\n# lb.transform([212]).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select only column we're interested in\ndf = df[['day','h', 'm', 'dow','geo_encoded','demand']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fbprophet\nfbprophet is the easiest tool for forecasting where you can model your solution quickly and sometime it's already good enough."},{"metadata":{"trusted":true},"cell_type":"code","source":"Prophet = fbprophet.Prophet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy = df.copy()\ndf_copy['year'] = 2018\ndf_copy['day_'] = df_copy['day'].apply(lambda x : x if x <= 31 else x-31 )\ndf_copy['month'] = df_copy['day'].apply(lambda x : 10 if x <= 31 else 11 )\ndf_copy['s'] = 0\ndf_copy['ds'] = pd.to_datetime(dict(year=df_copy.year, month=df_copy.month, day=df_copy.day_, hour=df_copy.h, minute=df_copy.m))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove columns that are not be used in the model\ndf_copy.drop(columns=['day','h','m', 's', 'dow', 'year', 'day_', 's', 'month'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fbprophet expect the y column\ndf_copy.rename(columns={'demand':'y'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_copy.groupby('geo_encoded').agg({'ds':'count'}).sort_values(by='ds', ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_copy = df_copy.sample(frac=0.1, random_state=1)\n# we'll take only one of the geohash area as taking entire dataset is extremely expensive\n# It would take 1h 28min 24s for training\n# so to save time I'll pick first 5 top areas\ndf_copy = df_copy[df_copy['geo_encoded'].isin([232,261,215,275,507])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sort by datetime\ndf_copy = df_copy.sort_values(by=['ds'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_n = int(0.8*len(df_copy))\ntrain = df_copy.iloc[:split_n,:]\ntest = df_copy.iloc[split_n:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mcmc_samples=300, changepoint_prior_scale=0.01 (enable this to get uncertainty bound)\nm = Prophet(seasonality_mode='multiplicative', \\\n            weekly_seasonality=True, \\\n            daily_seasonality=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nm.add_regressor('geo_encoded', mode='multiplicative')\nm.fit(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#future = m.make_future_dataframe(periods=5, freq='min')\nfuture = train[['ds','geo_encoded']]\nforecast = m.predict(future)\nrmse = np.sqrt(mean_squared_error(train['y'], forecast['yhat']))\nprint(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#future = m.make_future_dataframe(periods=5, freq='min')\nfuture = test[['ds','geo_encoded']]\nforecast = m.predict(future)\nrmse = np.sqrt(mean_squared_error(test['y'], forecast['yhat']))\nprint(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"future = df_copy[['ds','geo_encoded']]\nforecast = m.predict(future)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast['y'] = df_copy['y'].values\nforecast['yhat_lower'] =  np.clip(forecast.yhat_lower, 0, 1)\nforecast['yhat_upper'] =  np.clip(forecast.yhat_upper, 0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(14, 8))\nfirst = forecast.iloc[:split_n,:]\nax.plot(first.ds, first.y, 'ko', markersize=3)\nax.plot(first.ds, first.yhat, color='steelblue', lw=0.5)\n#ax.fill_between(first.ds, first.yhat_lower, first.yhat_upper, color='steelblue', alpha=0.3)\n\nsecond = forecast.iloc[split_n:,:]\nax.plot(second.ds, second.y, 'ro', markersize=3)\nax.plot(second.ds, second.yhat, color='coral', lw=0.5)\n#ax.fill_between(second.ds, second.yhat_lower, second.yhat_upper, color='coral', alpha=0.3)\nax.axvline(str(second.iloc[0]['ds']), color='0.8', alpha=0.9)\nax.grid(ls=':', lw=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean up \ndel df_copy\ndel forecast\ndel future\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network (LSTM)\n\nWe'll try to build a Encoder-Decoder LSTM model for multi-step forecasting with multivariate input data using methods explained in [machinelearningmastery.com](https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/)\n\nThe final average RMSE is significantly lower than fbprohet. Thought I didn't have time to verify the result. I'll leave it to later time.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll prototype using 100k sample since executing full 4 millions dataset will slow down our step\n# once we have good model we can gradually increase the size and observe the accuracy.\ndf_sample = df.sample(100000, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample = df_sample.sort_values(by=['day','h','m'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper for splitting data into train and test\ndef split_dataset(data, n_step=5):\n    n_train = int(0.8 * len(data))\n    train, test = data[:n_train,:], data[n_train:,:]\n    train = array(split(train, len(train)/n_step))\n    test = array(split(test, len(test)/n_step))\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(train, n_input, batch_size=100, epochs=50, verbose=1):\n    # prepare data\n    train_x, train_y = to_supervised(train, n_input)\n    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n    # reshape output into [samples, timesteps, features]\n    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n    model.add(RepeatVector(n_outputs))\n    model.add(LSTM(200, activation='relu', return_sequences=True))\n    model.add(TimeDistributed(Dense(100, activation='relu')))\n    model.add(TimeDistributed(Dense(1)))\n    model.compile(loss='mse', optimizer='adam')\n    # fit network\n    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_supervised(train, n_input, n_out=5):\n    # flatten data\n    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n    X, y = list(), list()\n    in_start = 0\n    #step over the entire history one time step at a time\n    for _ in range(len(data)):\n       # define the end of the input sequence\n       in_end = in_start + n_input\n       out_end = in_end + n_out\n       # ensure we have enough data for this instance\n       if out_end < len(data):\n          X.append(data[in_start:in_end, :])\n          y.append(data[in_end:out_end, 0])\n       # move along one time step\n       in_start += 1\n    return array(X), array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forecast(model, history, n_input):\n   # flatten data\n   data = array(history)\n   data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n   # retrieve last observations for input data\n   input_x = data[-n_input:, :]\n   # reshape into [1, n_input, n]\n   input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n   # forecast the next step\n   yhat = model.predict(input_x, verbose=0)\n   # we only want the vector forecast\n   yhat = yhat[0]\n   return yhat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate one or more forecasts against expected values\ndef evaluate_forecasts(actual, predicted):\n   scores = list()\n   # calculate an RMSE score for each day\n   for i in range(actual.shape[1]):\n      # calculate mse\n      mse = mean_squared_error(actual[:, i], predicted[:, i])\n      # calculate rmse\n      rmse = sqrt(mse)\n      # store\n      scores.append(rmse)\n   # calculate overall RMSE\n   s = 0\n   for row in range(actual.shape[0]):\n      for col in range(actual.shape[1]):\n         s += (actual[row, col] - predicted[row, col])**2\n   score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n   return score, scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf_sample[['day','h','m','dow','geo_encoded']] = scaler.fit_transform(df_sample[['day','h','m','dow','geo_encoded']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test\ntrain, test = split_dataset(df_sample.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"%%time\nn_input = 16 # the lagging \n# fit model\nmodel = build_model(train, n_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# history is a list of weekly data\nhistory = [x for x in train]\n# walk-forward validation over each steps\npredictions = list()\nfor i in range(len(test)):\n  # predict the week\n  yhat_sequence = forecast(model, history, n_input)\n  # store the predictions\n  predictions.append(yhat_sequence)\n  # get real observation and add to history for predicting the next week\n  history.append(test[i, :])\n# evaluate predictions days for each week\npredictions = array(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score, scores = evaluate_forecasts(test[:, :, 0], predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_scores = ', '.join(['%.1f' % s for s in scores])\nprint('lstm: [%.3f] %s' % (score, s_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"steps = [1,2,3,4,5]\nplt.plot(steps, scores, marker='o', label='lstm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}